{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f21a43-6977-47f3-a6d4-2d43fdbfa079",
   "metadata": {},
   "source": [
    "# https://pypi.org/project/duckduckgo-search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a56c4f-3950-41ff-80b1-ac5d3604b462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement unescape (from versions: none)\n",
      "ERROR: No matching distribution found for unescape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install duckduckgo_search --q\n",
    "!pip install textblob --q\n",
    "!pip install unidecode --q\n",
    "!pip install unescape --q\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c68f90a-31d0-4a53-8c57-61334cb70415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amarf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amarf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amarf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')     # jika kamu pakai lemmatizer\n",
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "908e14d5-111c-4956-a695-d1a06f9eb82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from duckduckgo_search import DDGS\n",
    "import pandas as pd\n",
    "import unidecode, re, textblob\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8400381-7c2b-4d87-8a06-64a6330951f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['facebook.com',\n",
       "  'instagram.com',\n",
       "  'x.com',\n",
       "  'linkedin.com',\n",
       "  'youtube.com',\n",
       "  'tiktok.com'],\n",
       " ['kompas.com', 'detik.com', 'liputan6.com', 'tribunnews.com'],\n",
       " ['\"sertifikasi\" \"halal\"'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medsos = \"facebook instagram x linkedin youtube tiktok\".split()\n",
    "medsos = [ms+\".com\" for ms in medsos]\n",
    "media = \"kompas detik liputan6 tribunnews\".split()\n",
    "media = [ms+\".com\" for ms in media]\n",
    "keywords = ['\"sertifikasi\" \"halal\"']\n",
    "\n",
    "medsos, media, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9387982f-3c62-4b9a-877d-22ce3a6760d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidur = 360 #delay 360 detik\n",
    "fData = 'data/Data_Halal.csv'\n",
    "fresume = \"data/sudah_diQuery.txt\"\n",
    "resume = False\n",
    "try:\n",
    "    with open(fresume, \"r\") as file:\n",
    "    sudahQuery = file.readlines()\n",
    "    if len(sudahQuery) < len(media) + len(medsos):\n",
    "        resume = True\n",
    "        df = pd.read_csv(fData)\n",
    "        for ms in medsos:\n",
    "            for kw in keywords:\n",
    "                qry = \"{} site:{}\".format(kw, ms)\n",
    "                print(qry)\n",
    "                if qry not in sudahQuery:\n",
    "                    belumDapetData = True\n",
    "                    hasil = None\n",
    "                    while belumDapetData:\n",
    "                        try:\n",
    "                            hasil = DDGS().text(qry, max_results=100)\n",
    "                        except:\n",
    "                            pass\n",
    "                        if not hasil:\n",
    "                            time.sleep(tidur) \n",
    "                        else:\n",
    "                            belumDapetData = False\n",
    "                    \n",
    "                    hasil = pd.DataFrame(hasil)\n",
    "                    hasil['source'] = ms\n",
    "                    df = pd.concat([df, hasil], axis=0)\n",
    "                    df.to_csv(fData, index=False)\n",
    "                    with open(fresume, \"a\") as file:\n",
    "                        file.write(qry)\n",
    "                    time.sleep(tidur) \n",
    "        for ms in media:\n",
    "            qry = 'halal site:{}'.format(ms)\n",
    "            print(qry)\n",
    "            if qry not in sudahQuery:            \n",
    "                belumDapetData = True\n",
    "                    hasil = None\n",
    "                    while belumDapetData:\n",
    "                        try:\n",
    "                            hasil = DDGS().text(qry, max_results=100)\n",
    "                        except:\n",
    "                            pass\n",
    "                        if not hasil:\n",
    "                            time.sleep(tidur) \n",
    "                        else:\n",
    "                            belumDapetData = False\n",
    "                hasil = pd.DataFrame(hasil)\n",
    "                hasil['source'] = ms\n",
    "                df = pd.concat([df, hasil], axis=0)\n",
    "                df.to_csv(fData, index=False)\n",
    "                with open(fresume, \"a\") as file:\n",
    "                        file.write(qry)\n",
    "                time.sleep(tidur)\n",
    "        \n",
    "except: #mulai dari awal\n",
    "    result = []\n",
    "    for ms in medsos:\n",
    "        for kw in keywords:\n",
    "            qry = \"{} site:{}\".format(kw, ms)\n",
    "            print(qry)\n",
    "            belumDapetData = True\n",
    "            hasil = None\n",
    "            while belumDapetData:\n",
    "                try:\n",
    "                    hasil = DDGS().text(qry, max_results=100)\n",
    "                except:\n",
    "                    pass\n",
    "                if not hasil:\n",
    "                    time.sleep(tidur) \n",
    "                else:\n",
    "                    belumDapetData = False\n",
    "                    for data in hasil:\n",
    "                        hasil['source'] = ms\n",
    "            results.extend(hasil)\n",
    "            time.sleep(tidur)    \n",
    "    for ms in media:\n",
    "        qry = 'halal site:{}'.format(ms)\n",
    "        print(qry)\n",
    "        belumDapetData = True\n",
    "        hasil = None\n",
    "        while belumDapetData:\n",
    "            try:\n",
    "                hasil = DDGS().text(qry, max_results=100)\n",
    "            except:\n",
    "                pass\n",
    "            if not hasil:\n",
    "                time.sleep(tidur) \n",
    "            else:\n",
    "                belumDapetData = False\n",
    "                for data in hasil:\n",
    "                    hasil['source'] = ms\n",
    "        results.extend(hasil)\n",
    "        time.sleep(tidur)\n",
    "    hasil = pd.DataFrame(result)\n",
    "    df.to_csv(fData, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a528a504-e8e0-46d9-9962-4ec971717779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ada', 'adalah', 'adanya', 'adapun', 'agak']\n"
     ]
    }
   ],
   "source": [
    "import unidecode, re, textblob\n",
    "from textblob import Word\n",
    "\n",
    "try:\n",
    "    f = 'stopwords_id.txt' # atau stopwords_en.txt\n",
    "    df=open(f,\"r\",encoding=\"utf-8\", errors='replace')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "stopWords = df.readlines();df.close()\n",
    "stopWords = [t.strip() for t in stopWords]\n",
    "print(stopWords[:5])\n",
    "stopWords = set(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84d1d48b-76a0-4e5e-afc1-09ef1d95bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing \n",
    "def txtPreprocessing(txt):\n",
    "    T = txt.lower() \n",
    "    T = re.sub(r'[^\\w]',' ',T) # remove symbols\n",
    "    T = TextBlob(T).words # Tokenisasi, T sekarang List\n",
    "    T = [Word(t).lemmatize() for t in T] # Lemma\n",
    "    T = [t for t in T if str(t) not in stopWords] # Untuk visualisasi data\n",
    "    return ' '.join(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e9a5cdc-4c48-46f6-aff3-9f9cdca72eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lppom mui bogor facebook lppom mui bogor indonesia 145 648 like 408 talking about this fanpage official lppom mui lppom mui lembaga sertifikasi halal lsh dunia terakreditasi sni iso iec',\n",
       " 'lph halal nusantara facebook sertifikasi halal tidak perspektif agama sudut pandang bisnis berbicara marketing branding produk irham sertifikasi halal alat pemasaran ampuh membantu bisnis menarik pelanggan negeri internasional',\n",
       " 'info sertifikasi halal indonesia facebook group medium silaturahmi pelaku usaha penyelia halal berbagi informasi terkait sertifikasi halal jaminan produk halal']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean'] = ''\n",
    "for i, d in tqdm(df.iterrows()):\n",
    "    post_bersih = txtPreprocessing(d.title + ' ' + d.body)\n",
    "    df.iloc[i, 'clean'] = post_bersih\n",
    "\n",
    "df.to_csv(fData, index=False) # Simpan hasil preProcessing\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
