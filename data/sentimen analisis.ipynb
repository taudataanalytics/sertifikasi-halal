{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Membaca file Excel dengan ekstensi .xls\n",
        "file_path = '/content/hasil_preprocessing.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Menampilkan beberapa baris pertama dari dataset\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBFOeeOHqlm6",
        "outputId": "1bf012f9-916c-4b66-98d5-e2ed0cef7a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  \\\n",
            "0                       LPPOM MUI | Bogor - Facebook   \n",
            "1                     LPH Halal Nusantara - Facebook   \n",
            "2        Info Sertifikasi Halal Indonesia - Facebook   \n",
            "3                Halal Corridor | Jakarta - Facebook   \n",
            "4  PERESMIAN ESQ HALAL CENTER ESQ mendukung progr...   \n",
            "\n",
            "                                                href  \\\n",
            "0           https://www.facebook.com/halalindonesia/   \n",
            "1        https://www.facebook.com/halalnusantara.id/   \n",
            "2  https://www.facebook.com/groups/1127431861260327/   \n",
            "3  https://www.facebook.com/profile.php/?id=61565...   \n",
            "4  https://www.facebook.com/AryGinanjarAgustian/v...   \n",
            "\n",
            "                                                body        source  \\\n",
            "0  LPPOM MUI, Bogor, Indonesia. 145,648 likes · 4...  facebook.com   \n",
            "1  pentingnya sertifikasi halal, tidak hanya dari...  facebook.com   \n",
            "2  Group ini adalah media silaturahmi antar Pelak...  facebook.com   \n",
            "3  Jun 1, 2025· 󰟠 Seru-seruan bareng Halal Corrid...  facebook.com   \n",
            "4  ESQ mendukung program Halal Indonesia. Selain ...  facebook.com   \n",
            "\n",
            "                                               clean  \n",
            "0  lppom mui bogor facebook lppom mui bogor indon...  \n",
            "1  lph halal nusantara facebook sertifikasi halal...  \n",
            "2  info sertifikasi halal indonesia facebook grou...  \n",
            "3  halal corridor jakarta facebook jun 1 2025 ser...  \n",
            "4  peresmian esq halal center esq mendukung progr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas openpyxl transformers scikit-learn nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDZGhA9BrPcr",
        "outputId": "afe43fae-96df-495d-b8c7-772a40329503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "# Unduh stopwords jika belum\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# ===== STEP 1: BACA DATA DAN HAPUS DUPLIKAT =====\n",
        "df = pd.read_csv(\"hasil_preprocessing.csv\")\n",
        "df = df.drop_duplicates(subset=\"clean\").reset_index(drop=True)\n",
        "\n",
        "# ===== STEP 2: ANALISIS SENTIMEN MENGGUNAKAN MODEL MULTIBAHASA BERT =====\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "def map_sentiment(label):\n",
        "    if label in [\"1 star\", \"2 stars\"]:\n",
        "        return \"Negatif\"\n",
        "    elif label == \"3 stars\":\n",
        "        return \"Netral\"\n",
        "    else:\n",
        "        return \"Positif\"\n",
        "\n",
        "texts = df[\"clean\"].astype(str).apply(lambda x: x[:512]).tolist()\n",
        "results = sentiment_pipeline(texts, batch_size=16)\n",
        "df[\"Sentimen\"] = [map_sentiment(r[\"label\"]) for r in results]\n",
        "\n",
        "# ===== STEP 3: EKSTRAK MAKSIMAL 8 TOPIK =====\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'\\d+', '', text.lower())\n",
        "    text = re.sub(r'\\b(?:' + '|'.join(stopwords.words('indonesian')) + r')\\b', '', text)\n",
        "    return text\n",
        "\n",
        "df[\"preprocessed\"] = df[\"clean\"].astype(str).apply(preprocess)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(df[\"preprocessed\"])\n",
        "\n",
        "k = 8  # maksimal 8 topik\n",
        "model = KMeans(n_clusters=k, random_state=42, n_init=10) # Added n_init to suppress warning\n",
        "df[\"Topik\"] = model.fit_predict(X)\n",
        "\n",
        "# Optional: Labelkan topik dengan kata kunci utama\n",
        "topik_label = []\n",
        "for i in range(k):\n",
        "    idx = (df[\"Topik\"] == i).values\n",
        "    tfidf_mean = X[idx].mean(axis=0).A1\n",
        "    keywords = [vectorizer.get_feature_names_out()[j] for j in tfidf_mean.argsort()[-3:][::-1]]\n",
        "    topik_label.append(\", \".join(keywords))\n",
        "\n",
        "df[\"Topik\"] = df[\"Topik\"].apply(lambda i: topik_label[i])\n",
        "\n",
        "# ===== STEP 4: SIMPAN HASIL KE FILE EXCEL =====\n",
        "df_final = df[[\"title\", \"href\", \"body\", \"source\", \"clean\", \"Sentimen\", \"Topik\"]]\n",
        "df_final.to_excel(\"hasil_sentimen_topik.xlsx\", index=False, sheet_name=\"Data\")\n",
        "\n",
        "# ===== STEP 5: CETAK RINGKASAN =====\n",
        "print(\"\\n📊 Ringkasan Sentimen:\")\n",
        "print(df[\"Sentimen\"].value_counts())\n",
        "\n",
        "print(\"\\n📚 Ringkasan Topik:\")\n",
        "print(df[\"Topik\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pFTFXqEriIw",
        "outputId": "357b9b17-4099-4f9b-a9fc-a026488ebc14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Ringkasan Sentimen:\n",
            "Sentimen\n",
            "Negatif    403\n",
            "Positif     93\n",
            "Netral      26\n",
            "Name: count, dtype: int64\n",
            "\n",
            "📚 Ringkasan Topik:\n",
            "Topik\n",
            "halal, indonesia, sertifikasi    138\n",
            "sertifikasi, halal, umkm         133\n",
            "produk, halal, bpjph              64\n",
            "makanan, islam, halal             47\n",
            "ayam, goreng, widuran             47\n",
            "restoran, tokyo, jepang           35\n",
            "bihalal, halal, tradisi           34\n",
            "bsi, international, expo          24\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}